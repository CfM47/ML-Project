|   Year | Article Name                                                                                                      | Authors                                                                                                                                           | Institution                                                                             | Image Type / Modality                                                                                                                                       | Resolution (Native / Input)                                                                                                                                                                                                     | Key Features / Color / Modality                                                                                                                                                                      | Used Datasets                                             | Input is Output of Another Model?   | Input Providing Model/Mechanism                                | Elaborated Explanation of Input Generation                                                                                                                                                                                                                                  | CNN (General/Dropout CNN)   | ResNet Models (18, 50, 101)   | Vision Transformer (ViT) Models (Base, Huge, ViT)   | DenseNet-121       | EfficientNet/B7    | MobileNet v2   | GoogLeNet   | CNN-T (Hybrid)     | VDSNet (Hybrid)   | VGG-16   | ZF   | AlexNet   | CapsNet   | SKAL   | NSGA-II/MOEA/AR-MOEA/SMS-EMOA (Pruning/Optimization)   | SVM (All Kernels)   | ELM                | Random Forest (RF)   | Accuracy/OA                                 | Precision / Micro_P       | Recall / Sensitivity (Sens.) / Micro_R        | F1 Score / Micro_F1      | F1/3 Score           | Specificity (Spec.)                             | ROC Area / AUC   | PRC Area / AUC-PR   | Time/Efficiency (H/S/ms/G/M)                   | Loss/Error                     |
|-------:|:------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------|:------------------------------------|:---------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------|:------------------------------|:----------------------------------------------------|:-------------------|:-------------------|:---------------|:------------|:-------------------|:------------------|:---------|:-----|:----------|:----------|:-------|:-------------------------------------------------------|:--------------------|:-------------------|:---------------------|:--------------------------------------------|:--------------------------|:----------------------------------------------|:-------------------------|:---------------------|:------------------------------------------------|:-----------------|:--------------------|:-----------------------------------------------|:-------------------------------|
|   2021 | Deep Learning in Image Classification using Residual Network (ResNet) Variants for Detection of Colorectal Cancer | Devvi Sarwindaa, Radifa Hilya Paradisaa, Alhadi Bustamama, Pinkie Anggiab                                                                         | Universitas Indonesia                                                                   | Histopathological tissue specimens (Colorectal gland images)                                                                                                | Native range: 567x430 to 775x522 pixels. Input size: 224 x 224 pixels (resized)                                                                                                                                                 | Images were converted to grayscale. Features enhanced using Contrast-Limited Adaptive Histogram Equalization (CLAHE). Pixel distance is 0.6 µm.                                                      | Warwick-QU Dataset                                        | No                                  | N/A                                                            | N/A                                                                                                                                                                                                                                                                         | No                          | Yes (50) (best results)       | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 88% (best results) (R50)                    | N/A                       | 96% (R18) / 93% (R50)                         | N/A                      | N/A                  | 92% (R50, 60%:40%) / 83% (R50)                  | N/A              | N/A                 | Run Time R50: 2.89s/epoch                      | N/A                            |
|   2024 | Machine Learning-Enabled Image Classification for Automated Electron Microscopy                                   | Alexandra L Day, Carolin B Wahl, Vishu Gupta, Roberto dos Reis, Wei-keng Liao, Chad A Mirkin, Vinayak P Dravid, Alok Choudhary, and Ankit Agrawal | Northwestern University                                                                 | High-Angle Annular Dark-Field (HAADF) images of nanoparticles, acquired using STEM.                                                                         | Native resolutions: 512 × 512 pixels (Data1, Data2) or 1,024 × 1,024 pixels (Data3). Input size: Trained on images resized to 256 × 256 pixels (due to memory constraints) or 512 × 512 pixels (for Data3 testing consistency). | Grayscale images (single channel). Normalized using zero-mean unit-variance normalization. Some frames were deliberately acquired slightly out of focus.                                             | Proprietary/Experimental HAADF Images                     | No                                  | Preprocessing Layers + EfficientNetB7 Block                    | The final input features were generated by feeding normalized and augmented HAADF images through preprocessing layers (e.g., Normalization, RandomTranslation, RandomFlip), which produced a 3-channel input to the EfficientNetB7 block.                                   | Yes                         | No                            | No                                                  | No                 | Yes (best results) | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 75.3%                                       | 96.2%                     | 70.2%                                         | N/A                      | 92.8% (best results) | N/A                                             | N/A              | N/A                 | Inf Time: 71–122 ms                            | N/A                            |
|   2021 | Dropout technique for image classification based on extreme learning machine                                      | Gangi Siva Nandini, A.P. Siva Kumar, Chidananda K                                                                                                 | N/A (Institution not explicitly named)                                                  | General computer vision images sourced from benchmark databases (MINIST, FACE, CIFAR, CIPHER).                                                              | Not explicitly specified, but input is highly dimensional .                                                                                                                                                                     | Highly varied due to backgrounds, viewpoints, and lighting. Preprocessed using Dense SIFT operation and Histogram Oriented Gradients (HOG).                                                          | MINIST, FACE, CIFAR, CIPHER Databases                     | Yes                                 | CNN Feature Mapping Stage + Dense SIFT/HOG                     | The low-level input features, initially generated using techniques like Dense SIFT and HOG, were processed hierarchically by the CNN Module (including convolution/pooling/dropout) to create the final feature maps input to the ELM classifier.                           | Yes                         | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | Yes (best results) | No                   | 1.0 (best results) (on MINIST)              | N/A                       | N/A                                           | N/A                      | N/A                  | N/A                                             | N/A              | N/A                 | Time: 1s (on MINIST)                           | 0.0 (best results) (on MINIST) |
|   2022 | Hybrid Architecture Based on CNN and Transformer for Strip Steel Surface Defect Classification                    | Shunfeng Li, Chunxue Wu, and Naixue Xiong                                                                                                         | University of Shanghai for Science and Technology; Sul Ross State University            | Images of hot rolled strip steel surface defects.                                                                                                           | Original size: 200 × 200 pixels. Input size: 224 × 224 pixels (scaled uniformly).                                                                                                                                               | Original images are grayscale, but converted to pseudo-color images (3 channels) using the JET color mapping algorithm for enhanced contrast and feature extraction.                                 | NEU-CLS Dataset                                           | Yes                                 | CNN Module (4 Convolutional Layers)                            | The CNN Module functioned as a feature extraction frontend, converting the pseudo-color input image into a compact feature map representation before it was patch embedded and sent to the Transformer encoder for global modeling.                                         | Yes                         | Yes (18)                      | Yes                                                 | No                 | No                 | Yes            | Yes         | Yes (best results) | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 99.17% (best results) (CNN-T)               | 99.21% (CNN-T)            | 99.17% (CNN-T)                                | 99.17% (CNN-T)           | N/A                  | N/A                                             | N/A              | N/A                 | FLOPs: 0.12 G (CNN-T) / Params: 0.48 M         | N/A                            |
|   2023 | Multi-Branch Deep Learning Framework for Land Scene Classification in Satellite Imagery                           | Sultan Daud Khan and Saleh Basalamah                                                                                                              | National University of Technology (Pakistan); Umm Al-Qura University (Saudi Arabia)     | Remote sensing images (high-resolution satellite and aerial images).                                                                                        | Varies by dataset (64x64 to 256x256 native). Input patches re-sized to 224 × 224 pixels.                                                                                                                                        | Images contain complex texture, cluttered background, extremely small objects, and large scale variations. Acquired from sources like the Sentinel-2A satellite and Google Earth.                    | UC-Merced, SIRI-WHU, EuroSAT Datasets                     | Yes                                 | Fusion Module (Score Combination from Two Branches)            | The final classification decision utilized the averaged prediction scores fused from two parallel branches: the Global Contextual Module (DenseNet+PPM) and the Local Feature Extraction Module (FCN+CNN), leveraging both holistic and regional information.               | No                          | Yes (50, 101)                 | No                                                  | Yes (best results) | Yes                | Yes            | Yes         | No                 | No                | Yes      | Yes  | Yes       | No        | Yes    | Yes                                                    | No                  | No                 | No                   | OA: 99.52% (best results) (D/D)             | 100.00% (Max class-wise)  | 100.00% (Max class-wise)                      | 100.00% (Max class-wise) | N/A                  | N/A                                             | N/A              | N/A                 | Train Time: 19.40 H (D/D) / 4.35 H (MobileNet) | N/A                            |
|   2023 | Vision Transformer Outperforms Deep Convolutional Neural Network-based Model in Classifying X-ray Images          | Om Uparkar, Jyoti Bharti, R. K. Pateriya, Rajeev Kumar Gupta, Ashutosh Sharma                                                                     | Maulana Azad National Institute of Technology (Bhopal, India)                           | Chest X-ray images (frontal-view) used for lung disease detection.                                                                                          | Original resolution: 1024 × 1024. Input size (VGG16 component): 224 × 224 in RGB (three channels)].                                                                                                                             | Images utilize transfer learning from ImageNet pre-trained weights. Classification influenced by concatenated auxiliary metadata (Age, Gender, X-ray view position PA/AP).                           | NIH chest X-rays dataset                                  | Yes                                 | ViT Encoder / VGG16 Features + Auxiliary Metadata              | The input for the final classification layer was formed by concatenating image features extracted by either the ViT Encoder or the VGG16 component with non-image Auxiliary Features (Age, Gender, X-ray View Position).                                                    | No                          | No                            | Yes (Huge) (best results)                           | No                 | No                 | No             | No          | No                 | Yes               | Yes      | No   | No        | Yes       | No     | No                                                     | No                  | No                 | No                   | 70.24% (best results) (ViT-Huge)            | 0.67 (ViT-Huge)           | 0.63 (ViT-Huge)                               | 0.65 (ViT-Huge)          | N/A                  | N/A                                             | N/A              | N/A                 | N/A                                            | N/A                            |
|   2024 | Fully Automated CTC Detection, Segmentation and Classification for Multi-Channel IF Imaging                       | Evan Schwab, Bharat Annaldas, Nisha Ramesh, Anna Lundberg, Vishal Shelke, Xinran Xu, Cole Gilbertson, Jiyun Byun, Ernest T. Lam                   | Epic Sciences (USA)                                                                     | Multi-channel Immunofluorescence (IF) images captured via Widefield fluorescence microscopy.                                                                | FOV size: 2040 × 2040 pixels. Input patches (U-Net): 512 × 512. Final classification on 24x24 pixel thumbnails.                                                                                                                 | Utilizes three channels: DAPI (nucleus), CK (Cytokeratin), and CD45/31 (non-CTC indicator). Classification relies on 122 extracted features (morphology, intensity, texture).                        | Internal Data (DefineMBC clinical diagnostic test images) | Yes                                 | Extracted Features (122 features)                              | The input consisted of 122 interpretable features (morphology, intensity, texture) quantitatively extracted from masks generated by the upstream detection and segmentation steps (LoG, Otsu’s method, 3-channel U-Net), which were fed into the RBF kernel SVM classifier. | No                          | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | Yes (best results)  | No                 | No                   | 97.8% (RBF SVM)                             | N/A                       | Sens: 99.1% (best results) (RBF SVM, Val Set) | N/A                      | N/A                  | 99.8% (RBF SVM, Training Set) / 96.9% (Val Set) | N/A              | N/A                 | Avg. Slide Time: 90 min                        | N/A                            |
|   2021 | The Active Segmentation Platform for Microscopic Image Classification and Segmentation                            | Sumit K. Vohra and Dimiter Prodanov                                                                                                               | Zuse Institute Berlin (ZIB); NERF (Neuroscience Research Flanders, Belgium)             | Microscopic images of cells and subcellular structures, including ssTEM (Transmission Electron Microscopy) (ISBI 2012) and fluorescent images (HeLa/HEp-2). | Varies by dataset: 512 × 512 (EM ISBI), 382 × 382 (HeLa), Variable size (HEp-2)                                                                                                                                                 | Images are typically 16 bit precision. Classification depends on extracted regional features (e.g., moments) and scale space pixel-features (differential invariants).                               | HeLa, HEp-2 Data Sets                                     | Yes                                 | Feature Vector (Regional Moments + Scale Space Pixel-features) | The SVM classifier operates on a specialized feature vector combining Regional Features (Legendre and Zernike moments) and Scale Space Pixel-features derived from Differential Geometry Filters (e.g., LoG, ALoG, Curvature), selectively chosen after feature selection.  | No                          | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | Yes (best results)  | No                 | Yes                  | TP Rate: 0.93 (best results) (SMO/SVM)      | 0.93 (SMO/SVM)            | 0.93 (SMO/SVM)                                | 0.93 (SMO/SVM)           | N/A                  | N/A                                             | 0.99 (SMO/SVM)   | 0.90 (SMO/SVM)      | N/A                                            | N/A                            |
|   2022 | Multi-stream Cell Segmentation with Low-level Cues for Multi-modality Images                                      | Wei Lou, Xinyi Yu, Chenyu Liu, Xiang Wan, Guanbin Li, Siqi Liu, Haofeng Li                                                                        | The Chinese University of Hong Kong (Shenzhen); Shenzhen Research Institute of Big Data | Multi-modal microscopy images for cell segmentation.                                                                                                        | Training used randomly sampled 512 × 512 image patches.                                                                                                                                                                         | Includes four modalities: Brightfield (BF), Fluorescent (Fluo), Phase-contrast (PC), and Differential Interference Contrast (DIC). Images exhibit various textures, patterns, and cell sizes/shapes. | Competition Dataset + Public datasets                     | Yes                                 | Unsupervised Classification Pipeline (Pseudo Labels)           | The ResNet18 classifier was trained using pseudo labels derived from an upstream pipeline that automatically grouped images into categories (Classes 0-3) based on low-level visual cues (e.g., color saturation, cell area/shape characteristics).                         | No                          | Yes (18)                      | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 97.91% (best results) (ResNet18 Classifier) | 99.48% (Class 0 accuracy) | N/A                                           | N/A                      | N/A                  | N/A                                             | N/A              | N/A                 | N/A                                            | N/A                            |