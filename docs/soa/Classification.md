|   Year | Article Name                                                                                                      | Authors                                                                                                                                           | Institution                                                                             | Image Type / Modality                                                                                                                                       | Resolution (Native / Input)                                                                                                                                                                                                     | Key Features / Color / Modality                                                                                                                                                                      | Used Datasets                                             | Input is Output of Another Model?   | Input Providing Model/Mechanism                                | Elaborated Explanation of Input Generation                                                                                                                                                                                                                                  | CNN (General/Dropout CNN)   | ResNet Models (18, 50, 101)   | Vision Transformer (ViT) Models (Base, Huge, ViT)   | DenseNet-121       | EfficientNet/B7    | MobileNet v2   | GoogLeNet   | CNN-T (Hybrid)     | VDSNet (Hybrid)   | VGG-16   | ZF   | AlexNet   | CapsNet   | SKAL   | NSGA-II/MOEA/AR-MOEA/SMS-EMOA (Pruning/Optimization)   | SVM (All Kernels)   | ELM                | Random Forest (RF)   | Accuracy/OA                                 | Precision / Micro_P       | Recall / Sensitivity (Sens.) / Micro_R        | F1 Score / Micro_F1      | F1/3 Score           | Specificity (Spec.)                             | ROC Area / AUC   | PRC Area / AUC-PR   | Time/Efficiency (H/S/ms/G/M)                   | Loss/Error                     |
|-------:|:------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------|:------------------------------------|:---------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------|:------------------------------|:----------------------------------------------------|:-------------------|:-------------------|:---------------|:------------|:-------------------|:------------------|:---------|:-----|:----------|:----------|:-------|:-------------------------------------------------------|:--------------------|:-------------------|:---------------------|:--------------------------------------------|:--------------------------|:----------------------------------------------|:-------------------------|:---------------------|:------------------------------------------------|:-----------------|:--------------------|:-----------------------------------------------|:-------------------------------|
|   2021 | Deep Learning in Image Classification using Residual Network (ResNet) Variants for Detection of Colorectal Cancer | Devvi Sarwindaa, Radifa Hilya Paradisaa, Alhadi Bustamama, Pinkie Anggiab                                                                         | Universitas Indonesia                                                                   | Histopathological tissue specimens (Colorectal gland images)                                                                                                | Native range: 567x430 to 775x522 pixels. Input size: 224 x 224 pixels (resized)                                                                                                                                                 | Images were converted to grayscale. Features enhanced using Contrast-Limited Adaptive Histogram Equalization (CLAHE). Pixel distance is 0.6 µm.                                                      | Warwick-QU Dataset                                        | No                                  | N/A                                                            | N/A                                                                                                                                                                                                                                                                         | No                          | Yes (50) (best results)       | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 88% (best results) (R50)                    | N/A                       | 96% (R18) / 93% (R50)                         | N/A                      | N/A                  | 92% (R50, 60%:40%) / 83% (R50)                  | N/A              | N/A                 | Run Time R50: 2.89s/epoch                      | N/A                            |
|   2024 | Machine Learning-Enabled Image Classification for Automated Electron Microscopy                                   | Alexandra L Day, Carolin B Wahl, Vishu Gupta, Roberto dos Reis, Wei-keng Liao, Chad A Mirkin, Vinayak P Dravid, Alok Choudhary, and Ankit Agrawal | Northwestern University                                                                 | High-Angle Annular Dark-Field (HAADF) images of nanoparticles, acquired using STEM.                                                                         | Native resolutions: 512 × 512 pixels (Data1, Data2) or 1,024 × 1,024 pixels (Data3). Input size: Trained on images resized to 256 × 256 pixels (due to memory constraints) or 512 × 512 pixels (for Data3 testing consistency). | Grayscale images (single channel). Normalized using zero-mean unit-variance normalization. Some frames were deliberately acquired slightly out of focus.                                             | Proprietary/Experimental HAADF Images                     | No                                  | Preprocessing Layers + EfficientNetB7 Block                    | The final input features were generated by feeding normalized and augmented HAADF images through preprocessing layers (e.g., Normalization, RandomTranslation, RandomFlip), which produced a 3-channel input to the EfficientNetB7 block.                                   | Yes                         | No                            | No                                                  | No                 | Yes (best results) | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 75.3%                                       | 96.2%                     | 70.2%                                         | N/A                      | 92.8% (best results) | N/A                                             | N/A              | N/A                 | Inf Time: 71–122 ms                            | N/A                            |
|   2021 | Dropout technique for image classification based on extreme learning machine                                      | Gangi Siva Nandini, A.P. Siva Kumar, Chidananda K                                                                                                 | N/A (Institution not explicitly named)                                                  | General computer vision images sourced from benchmark databases (MINIST, FACE, CIFAR, CIPHER).                                                              | Not explicitly specified, but input is highly dimensional .                                                                                                                                                                     | Highly varied due to backgrounds, viewpoints, and lighting. Preprocessed using Dense SIFT operation and Histogram Oriented Gradients (HOG).                                                          | MINIST, FACE, CIFAR, CIPHER Databases                     | Yes                                 | CNN Feature Mapping Stage + Dense SIFT/HOG                     | The low-level input features, initially generated using techniques like Dense SIFT and HOG, were processed hierarchically by the CNN Module (including convolution/pooling/dropout) to create the final feature maps input to the ELM classifier.                           | Yes                         | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | Yes (best results) | No                   | 1.0 (best results) (on MINIST)              | N/A                       | N/A                                           | N/A                      | N/A                  | N/A                                             | N/A              | N/A                 | Time: 1s (on MINIST)                           | 0.0 (best results) (on MINIST) |
|   2022 | Hybrid Architecture Based on CNN and Transformer for Strip Steel Surface Defect Classification                    | Shunfeng Li, Chunxue Wu, and Naixue Xiong                                                                                                         | University of Shanghai for Science and Technology; Sul Ross State University            | Images of hot rolled strip steel surface defects.                                                                                                           | Original size: 200 × 200 pixels. Input size: 224 × 224 pixels (scaled uniformly).                                                                                                                                               | Original images are grayscale, but converted to pseudo-color images (3 channels) using the JET color mapping algorithm for enhanced contrast and feature extraction.                                 | NEU-CLS Dataset                                           | Yes                                 | CNN Module (4 Convolutional Layers)                            | The CNN Module functioned as a feature extraction frontend, converting the pseudo-color input image into a compact feature map representation before it was patch embedded and sent to the Transformer encoder for global modeling.                                         | Yes                         | Yes (18)                      | Yes                                                 | No                 | No                 | Yes            | Yes         | Yes (best results) | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 99.17% (best results) (CNN-T)               | 99.21% (CNN-T)            | 99.17% (CNN-T)                                | 99.17% (CNN-T)           | N/A                  | N/A                                             | N/A              | N/A                 | FLOPs: 0.12 G (CNN-T) / Params: 0.48 M         | N/A                            |
|   2023 | Multi-Branch Deep Learning Framework for Land Scene Classification in Satellite Imagery                           | Sultan Daud Khan and Saleh Basalamah                                                                                                              | National University of Technology (Pakistan); Umm Al-Qura University (Saudi Arabia)     | Remote sensing images (high-resolution satellite and aerial images).                                                                                        | Varies by dataset (64x64 to 256x256 native). Input patches re-sized to 224 × 224 pixels.                                                                                                                                        | Images contain complex texture, cluttered background, extremely small objects, and large scale variations. Acquired from sources like the Sentinel-2A satellite and Google Earth.                    | UC-Merced, SIRI-WHU, EuroSAT Datasets                     | Yes                                 | Fusion Module (Score Combination from Two Branches)            | The final classification decision utilized the averaged prediction scores fused from two parallel branches: the Global Contextual Module (DenseNet+PPM) and the Local Feature Extraction Module (FCN+CNN), leveraging both holistic and regional information.               | No                          | Yes (50, 101)                 | No                                                  | Yes (best results) | Yes                | Yes            | Yes         | No                 | No                | Yes      | Yes  | Yes       | No        | Yes    | Yes                                                    | No                  | No                 | No                   | OA: 99.52% (best results) (D/D)             | 100.00% (Max class-wise)  | 100.00% (Max class-wise)                      | 100.00% (Max class-wise) | N/A                  | N/A                                             | N/A              | N/A                 | Train Time: 19.40 H (D/D) / 4.35 H (MobileNet) | N/A                            |
|   2023 | Vision Transformer Outperforms Deep Convolutional Neural Network-based Model in Classifying X-ray Images          | Om Uparkar, Jyoti Bharti, R. K. Pateriya, Rajeev Kumar Gupta, Ashutosh Sharma                                                                     | Maulana Azad National Institute of Technology (Bhopal, India)                           | Chest X-ray images (frontal-view) used for lung disease detection.                                                                                          | Original resolution: 1024 × 1024. Input size (VGG16 component): 224 × 224 in RGB (three channels)].                                                                                                                             | Images utilize transfer learning from ImageNet pre-trained weights. Classification influenced by concatenated auxiliary metadata (Age, Gender, X-ray view position PA/AP).                           | NIH chest X-rays dataset                                  | Yes                                 | ViT Encoder / VGG16 Features + Auxiliary Metadata              | The input for the final classification layer was formed by concatenating image features extracted by either the ViT Encoder or the VGG16 component with non-image Auxiliary Features (Age, Gender, X-ray View Position).                                                    | No                          | No                            | Yes (Huge) (best results)                           | No                 | No                 | No             | No          | No                 | Yes               | Yes      | No   | No        | Yes       | No     | No                                                     | No                  | No                 | No                   | 70.24% (best results) (ViT-Huge)            | 0.67 (ViT-Huge)           | 0.63 (ViT-Huge)                               | 0.65 (ViT-Huge)          | N/A                  | N/A                                             | N/A              | N/A                 | N/A                                            | N/A                            |
|   2024 | Fully Automated CTC Detection, Segmentation and Classification for Multi-Channel IF Imaging                       | Evan Schwab, Bharat Annaldas, Nisha Ramesh, Anna Lundberg, Vishal Shelke, Xinran Xu, Cole Gilbertson, Jiyun Byun, Ernest T. Lam                   | Epic Sciences (USA)                                                                     | Multi-channel Immunofluorescence (IF) images captured via Widefield fluorescence microscopy.                                                                | FOV size: 2040 × 2040 pixels. Input patches (U-Net): 512 × 512. Final classification on 24x24 pixel thumbnails.                                                                                                                 | Utilizes three channels: DAPI (nucleus), CK (Cytokeratin), and CD45/31 (non-CTC indicator). Classification relies on 122 extracted features (morphology, intensity, texture).                        | Internal Data (DefineMBC clinical diagnostic test images) | Yes                                 | Extracted Features (122 features)                              | The input consisted of 122 interpretable features (morphology, intensity, texture) quantitatively extracted from masks generated by the upstream detection and segmentation steps (LoG, Otsu’s method, 3-channel U-Net), which were fed into the RBF kernel SVM classifier. | No                          | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | Yes (best results)  | No                 | No                   | 97.8% (RBF SVM)                             | N/A                       | Sens: 99.1% (best results) (RBF SVM, Val Set) | N/A                      | N/A                  | 99.8% (RBF SVM, Training Set) / 96.9% (Val Set) | N/A              | N/A                 | Avg. Slide Time: 90 min                        | N/A                            |
|   2021 | The Active Segmentation Platform for Microscopic Image Classification and Segmentation                            | Sumit K. Vohra and Dimiter Prodanov                                                                                                               | Zuse Institute Berlin (ZIB); NERF (Neuroscience Research Flanders, Belgium)             | Microscopic images of cells and subcellular structures, including ssTEM (Transmission Electron Microscopy) (ISBI 2012) and fluorescent images (HeLa/HEp-2). | Varies by dataset: 512 × 512 (EM ISBI), 382 × 382 (HeLa), Variable size (HEp-2)                                                                                                                                                 | Images are typically 16 bit precision. Classification depends on extracted regional features (e.g., moments) and scale space pixel-features (differential invariants).                               | HeLa, HEp-2 Data Sets                                     | Yes                                 | Feature Vector (Regional Moments + Scale Space Pixel-features) | The SVM classifier operates on a specialized feature vector combining Regional Features (Legendre and Zernike moments) and Scale Space Pixel-features derived from Differential Geometry Filters (e.g., LoG, ALoG, Curvature), selectively chosen after feature selection.  | No                          | No                            | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | Yes (best results)  | No                 | Yes                  | TP Rate: 0.93 (best results) (SMO/SVM)      | 0.93 (SMO/SVM)            | 0.93 (SMO/SVM)                                | 0.93 (SMO/SVM)           | N/A                  | N/A                                             | 0.99 (SMO/SVM)   | 0.90 (SMO/SVM)      | N/A                                            | N/A                            |
|   2022 | Multi-stream Cell Segmentation with Low-level Cues for Multi-modality Images                                      | Wei Lou, Xinyi Yu, Chenyu Liu, Xiang Wan, Guanbin Li, Siqi Liu, Haofeng Li                                                                        | The Chinese University of Hong Kong (Shenzhen); Shenzhen Research Institute of Big Data | Multi-modal microscopy images for cell segmentation.                                                                                                        | Training used randomly sampled 512 × 512 image patches.                                                                                                                                                                         | Includes four modalities: Brightfield (BF), Fluorescent (Fluo), Phase-contrast (PC), and Differential Interference Contrast (DIC). Images exhibit various textures, patterns, and cell sizes/shapes. | Competition Dataset + Public datasets                     | Yes                                 | Unsupervised Classification Pipeline (Pseudo Labels)           | The ResNet18 classifier was trained using pseudo labels derived from an upstream pipeline that automatically grouped images into categories (Classes 0-3) based on low-level visual cues (e.g., color saturation, cell area/shape characteristics).                         | No                          | Yes (18)                      | No                                                  | No                 | No                 | No             | No          | No                 | No                | No       | No   | No        | No        | No     | No                                                     | No                  | No                 | No                   | 97.91% (best results) (ResNet18 Classifier) | 99.48% (Class 0 accuracy) | N/A                                           | N/A                      | N/A                  | N/A                                             | N/A              | N/A                 | N/A                                            | N/A                            |
| 2023 | Multi-Task Learning of Scanning Electron Microscopy and Synthetic Thermal Tomography Images for Detection of Defects in Additively Manufactured Metals | Scott, S.; Chen, W.-Y.; Heifetz, A. | Nuclear Science and Engineering Division, Argonne National Laboratory; Dept. of Civil and Environmental Engineering, Duke University | Synthetic Thermal Tomography (TT); Experimental Scanning Electron Microscopy (SEM) | TT: 500 × 342 pixels. SEM Input: 224 × 224 pixels. SEM Native: 15 nm/pixel. | Multi-Task Learning (MTL) for regression/classification (TT) and segmentation (SEM). Shared U-Net encoder. TT images are pseudocolor (thermal effusivity). Dropout (0.2 probability) in TT decoder. | Synthetic TT images (329 elliptical defect images in SS316). Experimental SEM images (49 images with defects, from 212 total of SS316L). | Yes (for TT) | Thermal Tomography (TT) algorithm processing Pulsed Infrared Thermography (PIT) data. | PIT data simulated with MATLAB heat transfer simulations. The TT algorithm reconstructs thermal effusivity $e(x,y)$. | Yes (U-Net based CNN uses double convolutional layers and max-pooling). Dropout used. | No | No | No | No | No | No | No | No | No | No | No | No | No | No | No | No | N/A (Regression/Segmentation tasks) | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Training occurred over 500 epochs. | Test MSE: 38.54 (MTL). Test AE: 0.47 (MTL). Testing BCE Loss: 0.03 (MTL). Testing IoU: 0.87 (MTL). |
| 2023 | Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers | Huang, C.-F.; Sieg, K.; Karlinksy, L.; Flores, N.; Sheraw, R.; Zhang, X. | IBM Research, Albany, NY; IBM Research, Cambridge MA; IBM Research, Yorktown Heights, NY | SEM images of wafer defects (semiconductors) | Input: 224 × 224. Native/Cropped: 340 × 340 (from originals 680x680 or 480x480). | Automatic Defect Classification (ADC). Transfer Learning. Semi-Supervised Learning using Pseudo-Labels. | Semiconductor wafer defect data from IBM Albany fab (over 7400 total images, 11 defect types). | No | Fab system tools/Electron beam imaging. | Data downloaded from inspection layers, cropped to remove annotations, and manually labeled. | Mentioned as dominant model, but ViT is focus. | N/A | Yes (Vision Transformer/ViT approach). DinoV2 (self-supervised pre-trained ViT model) applied. | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Yes (k-NN used for preliminary performance using Gaussian kernel). | N/A | N/A | Classification Accuracy: Over 90% (with < 15 images/defect for finetuning). Test accuracy (pseudo-labels): 93.3% to 100%. | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Potential for faster turnaround time and efficient computation. ViTs are more computationally expensive than CNNs. | N/A |
| 2020 | Deep Learning for Classification of the Chemical Composition of Particle Defects on Semiconductor Wafers | O’Leary, J.; Sawlani, K.; Mesbah, A. | Department of Chemical and Biomolecular Engineering, University of California, Berkeley, CA; Lam Research Corporation, Fremont, CA | SEM images of wafer defects; Energy-Dispersive X-ray (EDX) spectroscopy data. | Input: 140 × 140 pixels (cropped). Native: 480 × 480 pixels. | Classification of chemical composition. Hybrid CNN: fuses spectral EDX data with CNN's fully connected layer. Dropout (50%) used. Canny edge detection for outlier detection. | 5761 real semiconductor defects (8 classes) provided by Lam Research Corporation. | No | Review SEM/EDX tool. | Data obtained via optical scattering tool for location/size, then reviewed by SEM/EDX tool. Images cropped to 140x140. | Yes (Deep CNN, 5 convolutional blocks, architecture based on VGGNet). Dropout (50%) in FC layers. | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Yes (VGGNet-based architecture used for convolutional/pooling layers). | No | Tested, performed poorly. | N/A | N/A | N/A | Referenced. | N/A | N/A | Combined CNN: Top-1 Accuracy: 82.1%. Top-3 Accuracy: 99.2%. | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Training speed: 0.264 s/defect/epoch/GPU. CNN TLv2: Relative training time 0.15. | Categorical cross-entropy loss. Mean Validation Accuracy: 83.0%. |
| 2020 | Identification and Classification of Atmospheric Particles Based on SEM Images Using Convolutional Neural Network with Attention Mechanism | Yin, C.; Cheng, X.; Liu, X.; Zhao, M. | College of Electrical Engineering and Automation, Shandong University of Science and Technology; Hangzhou Hikvision Digital Technology Co., Ltd. | SEM images of atmospheric particles (PM2.5). | Magnification 20,000 times. Processed/Cropped/Rotated (3469 images). | Classification of morphological characteristics (fibrous, flocculent, spherical, mineral). Attention Mechanism integrated into CNN (Attention-CNN). | Qingdao 2016–2018 database (3469 single-particle SEM images). | No | US FEI Nova Nano SEM 450 (high vacuum mode). | Samples collected on filter membranes, coated with platinum by ion sputtering, and SEM images obtained. Images processed via cropping and rotating. | Yes (Attention-CNN model uses convolution, pooling, and full connection layers). | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Referenced as a feature extractor in related work. | N/A | N/A | N/A | N/A | N/A | Yes (used for comparison). | N/A | N/A | Classification Accuracy (Range): 94.33% to 98.56%. | Fibrous: 98.33%. Flocculent: 96.67%. Spherical: 94.17%. Mineral: 84.02%. | Fibrous: 96.71%. Flocculent: 98.31%. Spherical: 89.74%. Mineral: 88.46%. | Fibrous: 97.51%. Flocculent: 97.48%. Spherical: 91.90%. Mineral: 86.18%. | N/A | Fibrous: 99.32%. Flocculent: 98.66%. Spherical: 98.39%. Mineral: 95.80%. | N/A | N/A | Training time set to 200 iterations. | Cross-entropy cost function used as loss function. |
| 2021 | Deep Learning Based Instance Segmentation of Titanium Dioxide Particles in the Form of Agglomerates in Scanning Electron Microscopy | Monchot, P.; Coquelin, L.; Guerroudj, K.; Feltin, N.; Delvallée, A.; Crouzier, L.; Fischer, N. | National Laboratory of Metrology and Testing (LNE), France | SEM images of Titanium Dioxide ($TiO_2$) particle agglomerates. Grayscale (C=1 channel). | W=2048, L=1536 pixels. Mini-mask size: 96 pixels. | Instance Segmentation (Mask R-CNN). Transfer Learning (from MS COCO). Application-specific Data Augmentation. | Function-specific database (77 real images manually segmented, 5947 particles). Test set: 19 images (3741 particles). | No | Scanning Electron Microscopy (SEM). | Images acquired with SEM. Agglomerates extracted, rotated/flipped, and randomly positioned on SEM backgrounds (data augmentation). | Yes (FCN used within Mask R-CNN structure). | Yes (ResNet is the standard backbone, coupled with FPN). | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Referenced as a classic method. | N/A | N/A | mAP: 60.6. Detection rate: 83.80% (all particles). AP50: 84.6. | N/A | N/A | N/A | N/A | N/A | N/A | N/A | Total segmentation time: 110 s (for 3741 particles). Per particle: 0.035 s. | Mean DICE coefficient: 0.936 (All particles). DICE coefficient: 0.95 (over useful particles). |